#+TITLE:     State of the Art Video Compression
#+AUTHOR:    Zachary Harvey
#+EMAIL:     harveyz1@sunypoly.edu
#+DATE:      10-01-2024
#+EXPORT_FILE_NAME: harveyz1_presentation
#+DESCRIPTION: Presentation 1 for CS548 Video
#+KEYWORDS: 
#+LANGUAGE:  en
#+OPTIONS:   H:1 num:nil toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t tex:imagemagick
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+HTML_LINK_UP:
#+HTML_LINK_HOME:
#+BEAMER_THEME: Dresden [height=14pt]

#+startup: beamer
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [bigger]
#+LATEX_HEADER: \usepackage{biblatex}
#+LATEX_HEADER: \usepackage{textcomp}
#+LATEX_HEADER: \addbibresource{references.bib}
#+LATEX_HEADER: \setbeamertemplate{navigation symbols}{}


* Video Compression
https://mnd-assets.mynewsdesk.com/image/upload/c_limit,dpr_auto,f_auto,h_700,q_auto,w_auto/hp3atcj4hf9npilgyu37.jpg

* Deep Contextual Video Compression \cite{NEURIPS2021_96b250a9}
[[./dcvcFramework.png]]
The network learns condition rather than the frames or residue encoding.
The network generates context instead of a predicted from.

The context is in the feature domain to help provide a larger capacity of context over the standard pixel domain.

* DCVC con't
[[./visual_examples.png]]
The four channel examples in context \={x}_t. 

* DCVC con't
This method results in being better at encoding motion verses previous residue coding. We can use the network to learn the correlation between x_t and ={x}_t which removes the redundancy rather than using fixed subtraction.

* Modifications
[[./noises.png]]
We make new datasets by adding noise.
